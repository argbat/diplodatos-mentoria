{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSkjVlG7Unj5"
   },
   "source": [
    "# Curación - Relaciones Temporales en Textos Médicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZYbJyDBUnj9"
   },
   "source": [
    "### Recorrido por los datos\n",
    "Vamos a trabajar primero explorando los datos de ejemplo que se proveen con el challenge.\n",
    "Para eso hacemos uso de la función lift_data(), que extrae anotaciones de relaciones y las combina en una lista junto con el texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eomaLrC9Unj_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33635"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers import lift_dataset\n",
    "# lift_dataset devuelve un iterador\n",
    "dataset = list(lift_dataset('../dataset/2012-07-15.original-annotation.release'))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EVENT=\"Admission\" 1:0 1:0',\n",
       "  'TIMEX3=\"2014-03-31\" 2:0 2:0',\n",
       "  'type=\"SIMULTANEOUS\"\\n',\n",
       "  'Admission Date :\\n',\n",
       "  '193.xml.tlink'],\n",
       " ['EVENT=\"Discharge\" 3:0 3:0',\n",
       "  'TIMEX3=\"2014-04-09\" 4:0 4:0',\n",
       "  'type=\"SIMULTANEOUS\"\\n',\n",
       "  'Discharge Date :\\n',\n",
       "  '193.xml.tlink'],\n",
       " ['EVENT=\"admitted\" 9:4 9:4',\n",
       "  'EVENT=\"Nantucket Cottage Hospital\" 9:11 9:13',\n",
       "  'type=\"OVERLAP\"\\n',\n",
       "  'The patient has been admitted 5 x in the past to Nantucket Cottage Hospital since 11-30 for encephalopathy .\\n',\n",
       "  '193.xml.tlink'],\n",
       " ['TIMEX3=\"2014-04-04\" 48:1 48:1',\n",
       "  'EVENT=\"labs\" 48:3 48:3',\n",
       "  'type=\"SIMULTANEOUS\"\\n',\n",
       "  'On 2014-04-04 , labs were as follows :\\n',\n",
       "  '193.xml.tlink'],\n",
       " ['EVENT=\"WBC\" 49:0 49:0',\n",
       "  'TIMEX3=\"2014-04-04\" 48:1 48:1',\n",
       "  'type=\"SIMULTANEOUS\"\\n',\n",
       "  'WBC of 8.3 ; hematocrit of 30.6 ; platelets 69 ; sodium 132 ; 3.9 , 102 , 22 , serum creatinine of 13 and 1.1 .\\n',\n",
       "  '193.xml.tlink']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_1</th>\n",
       "      <th>event_2</th>\n",
       "      <th>relationship</th>\n",
       "      <th>text</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EVENT=\"Admission\" 1:0 1:0</td>\n",
       "      <td>TIMEX3=\"2014-03-31\" 2:0 2:0</td>\n",
       "      <td>type=\"SIMULTANEOUS\"\\n</td>\n",
       "      <td>Admission Date :\\n</td>\n",
       "      <td>193.xml.tlink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EVENT=\"Discharge\" 3:0 3:0</td>\n",
       "      <td>TIMEX3=\"2014-04-09\" 4:0 4:0</td>\n",
       "      <td>type=\"SIMULTANEOUS\"\\n</td>\n",
       "      <td>Discharge Date :\\n</td>\n",
       "      <td>193.xml.tlink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVENT=\"admitted\" 9:4 9:4</td>\n",
       "      <td>EVENT=\"Nantucket Cottage Hospital\" 9:11 9:13</td>\n",
       "      <td>type=\"OVERLAP\"\\n</td>\n",
       "      <td>The patient has been admitted 5 x in the past ...</td>\n",
       "      <td>193.xml.tlink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TIMEX3=\"2014-04-04\" 48:1 48:1</td>\n",
       "      <td>EVENT=\"labs\" 48:3 48:3</td>\n",
       "      <td>type=\"SIMULTANEOUS\"\\n</td>\n",
       "      <td>On 2014-04-04 , labs were as follows :\\n</td>\n",
       "      <td>193.xml.tlink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EVENT=\"WBC\" 49:0 49:0</td>\n",
       "      <td>TIMEX3=\"2014-04-04\" 48:1 48:1</td>\n",
       "      <td>type=\"SIMULTANEOUS\"\\n</td>\n",
       "      <td>WBC of 8.3 ; hematocrit of 30.6 ; platelets 69...</td>\n",
       "      <td>193.xml.tlink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EVENT=\"hematocrit\" 49:4 49:4</td>\n",
       "      <td>TIMEX3=\"2014-04-04\" 48:1 48:1</td>\n",
       "      <td>type=\"SIMULTANEOUS\"\\n</td>\n",
       "      <td>WBC of 8.3 ; hematocrit of 30.6 ; platelets 69...</td>\n",
       "      <td>193.xml.tlink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         event_1  \\\n",
       "0      EVENT=\"Admission\" 1:0 1:0   \n",
       "1      EVENT=\"Discharge\" 3:0 3:0   \n",
       "2       EVENT=\"admitted\" 9:4 9:4   \n",
       "3  TIMEX3=\"2014-04-04\" 48:1 48:1   \n",
       "4          EVENT=\"WBC\" 49:0 49:0   \n",
       "5   EVENT=\"hematocrit\" 49:4 49:4   \n",
       "\n",
       "                                        event_2           relationship  \\\n",
       "0                   TIMEX3=\"2014-03-31\" 2:0 2:0  type=\"SIMULTANEOUS\"\\n   \n",
       "1                   TIMEX3=\"2014-04-09\" 4:0 4:0  type=\"SIMULTANEOUS\"\\n   \n",
       "2  EVENT=\"Nantucket Cottage Hospital\" 9:11 9:13       type=\"OVERLAP\"\\n   \n",
       "3                        EVENT=\"labs\" 48:3 48:3  type=\"SIMULTANEOUS\"\\n   \n",
       "4                 TIMEX3=\"2014-04-04\" 48:1 48:1  type=\"SIMULTANEOUS\"\\n   \n",
       "5                 TIMEX3=\"2014-04-04\" 48:1 48:1  type=\"SIMULTANEOUS\"\\n   \n",
       "\n",
       "                                                text    source_file  \n",
       "0                                 Admission Date :\\n  193.xml.tlink  \n",
       "1                                 Discharge Date :\\n  193.xml.tlink  \n",
       "2  The patient has been admitted 5 x in the past ...  193.xml.tlink  \n",
       "3           On 2014-04-04 , labs were as follows :\\n  193.xml.tlink  \n",
       "4  WBC of 8.3 ; hematocrit of 30.6 ; platelets 69...  193.xml.tlink  \n",
       "5  WBC of 8.3 ; hematocrit of 30.6 ; platelets 69...  193.xml.tlink  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)\n",
    "df.columns = ['event_1', 'event_2', 'relationship', 'text', 'source_file']\n",
    "df[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqZqwsmiUnkH"
   },
   "source": [
    "## Tokenización\n",
    "Tokenizamos los datos y calculamos el tamaño de vocabulario, para ello se puede usar el siguiente código ejemplo con SpaCy. (reemplazar esto con el código que hace la tokenización sobre la variable dataset del punto anterior).\n",
    "El objetivo aquí es achicar el tamaño del vocabulario.\n",
    "El texto en el punto anterior está en el penúltimo elemento de cada lista que comprende cada punto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssh4ukE8UnkK"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "df['text_as_doc'] = df['text'].apply(lambda x: spacy_nlp(x))\n",
    "df['text_tokens'] = df['text_as_doc'].apply(lambda doc: [token.text for token in doc])\n",
    "df['text_tokens'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-csP3SZUnkT"
   },
   "outputs": [],
   "source": [
    "# imprimimos el tamaño del vocabulario\n",
    "vocab = []\n",
    "for tokens in df['text_tokens']:\n",
    "  vocab.extend(tokens)\n",
    "vocab = set(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "doRhZqSNUnka"
   },
   "source": [
    "## Lematización\n",
    "Seguimos usando SpaCy, esta vez para obtener lemas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWovwWnyUnkb"
   },
   "outputs": [],
   "source": [
    "df['text_lemma'] = df['text_as_doc'].apply(lambda doc: [word.lemma_ for word in doc])\n",
    "df['text_lemma'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kh4DZD7NUnkg"
   },
   "source": [
    "## Sigamos normalizando\n",
    "Algunas ideas, los números se pueden reemplazar por un token especial, _NUM_, las fechas también. \n",
    "Utilizar el módulo de regexes de Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(df, col_name, patterns):\n",
    "    count = sum([len(df[df[col_name].str.contains(pattern, regex=True) == True].index) for pattern in patterns])\n",
    "    return count\n",
    "\n",
    "def replace_with_patterns(df, col_name, patterns, replace_with):\n",
    "    for pattern in patterns:\n",
    "        df[col_name] = df[col_name].str.replace(pattern, replace_with, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rknH3E8WUnkj"
   },
   "outputs": [],
   "source": [
    "# A partir de los lemmas rearmamos el texto bajo una nueva columna que se normalizara.\n",
    "df['text_norm'] = df['text_lemma'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# lower case\n",
    "df['text_norm'] = df['text_norm'].str.lower()\n",
    "\n",
    "# Cantidad de numeros.\n",
    "numbers_patterns = ['\\d{1,9}\\.{0,1}\\d{0,4}']\n",
    "print('Cantidad de numeros en texto original: ', count(df, 'text', numbers_patterns))\n",
    "\n",
    "# Cantidad de fechas.\n",
    "dates_patterns = ['^\\d{4}-\\d{2}-\\d{2}$', '\\d{4}-\\d{2}-\\d{2}', '\\d{1,2}/\\d{1,2}/\\d{2}', '\\d{1}-\\d{1}']\n",
    "print('Cantidad de fechas en texto original: ', count(df, 'text', dates_patterns))\n",
    "\n",
    "# normalizamos fechas\n",
    "dates_patterns = ['^\\d{4}\\s-\\s\\d{2}\\s-\\s\\d{2}$', '\\d{4}\\s-\\s\\d{2}\\s-\\s\\d{2}', '\\d{1,2}\\s/\\s\\d{1,2}\\s/\\s\\d{2}', '\\d{1}\\s-\\s\\d{1}']\n",
    "replace_with_patterns(df, 'text_norm', dates_patterns, 'DATE')\n",
    "print('Cantidad de fechas en texto normalizado: ', count(df, 'text_norm', dates_patterns))\n",
    "\n",
    "# normalizamos números\n",
    "replace_with_patterns(df, 'text_norm', numbers_patterns, 'NUM')\n",
    "print('Cantidad de numeros en texto normalizado: ', count(df, 'text_norm', numbers_patterns))\n",
    "\n",
    "print(df['text_norm'][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removemos caracter de nueva linea\n",
    "print(len(df[df['text'].str.contains('\\n', regex=False) == True]))\n",
    "print(df['text'][:1])\n",
    "df['text_norm'] = df['text_norm'].str.strip()\n",
    "print(len(df[df['text_norm'].str.contains('\\n', regex=False) == True]))\n",
    "print(df['text_norm'][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-06SlheHUnkp"
   },
   "source": [
    "## Volvemos a calcular el tamaño del vocabulario\n",
    "El tamaño debería haberse reducido ya que hemos colapsado cosas distintas hacia los mismos tokens. \n",
    "Más adelante vamos a ver porque esto es interesante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QO_NscmlUnkq"
   },
   "outputs": [],
   "source": [
    "# calculamos el tamaño del vocabulario luego de normalizar\n",
    "df['text_norm_as_doc'] = df['text_norm'].apply(lambda x: spacy_nlp(x))\n",
    "df['text_norm_tokens'] = df['text_norm_as_doc'].apply(lambda doc: [token.text for token in doc])\n",
    "vocab = []\n",
    "for tokens in df['text_norm_tokens']:\n",
    "  vocab.extend(tokens)\n",
    "vocab = set(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad de relaciones.\n",
    "df['relationship'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpiamos para que quede solo el nombre de la realcion.\n",
    "df['relationship_norm'] = df['relationship']\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"BEFORE\"\\n', 'BEFORE', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"OVERLAP\"\\n', 'OVERLAP', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"SIMULTANEOUS\"\\n', 'SIMULTANEOUS', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"simultaneous\"\\n', 'SIMULTANEOUS', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"BEFORE_OVERLAP\"\\n', 'BEFORE_OVERLAP', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"AFTER\"\\n', 'AFTER', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"DURING\"\\n', 'DURING', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"BEGUN_BY\"\\n', 'BEGUN_BY', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"ENDED_BY\"\\n', 'ENDED_BY', regex=True)\n",
    "df.relationship_norm = df.relationship_norm.str.replace('type=\"\"\\n', '', regex=True)\n",
    "# Hay un relacion con nombe vacio, la vamos a eliminar.\n",
    "df = df.drop(df[df.relationship_norm == ''].index)\n",
    "print(df.relationship_norm.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de eventos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el nombre del tipo del evento del texto del evento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo.\n",
    "df['event_1_type'] = df.apply(\n",
    "    lambda row: 'EVENT' if 'EVENT' in row['event_1'] else 'TIMEX3',\n",
    "    axis=1\n",
    ")\n",
    "df['event_2_type'] = df.apply(\n",
    "    lambda row: 'EVENT' if 'EVENT' in row['event_2'] else 'TIMEX3',\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto.\n",
    "df['event_1_norm'] = df['event_1']\n",
    "df.event_1_norm = df.event_1_norm.str.extract(r'(\\\".*\\\")', expand=False)\n",
    "df.event_1_norm = df.event_1_norm.str.strip('\"')\n",
    "df.event_1_norm = df.event_1_norm.str.strip()\n",
    "df.event_1_norm = df.event_1_norm.str.lower()\n",
    "\n",
    "df['event_2_norm'] = df['event_2']\n",
    "df.event_2_norm = df.event_2_norm.str.extract(r'(\\\".*\\\")', expand=False)\n",
    "df.event_2_norm = df.event_2_norm.str.strip('\"')\n",
    "df.event_2_norm = df.event_2_norm.str.strip()\n",
    "df.event_2_norm = df.event_2_norm.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos fechas y numeros en el texto de evento.\n",
    "\n",
    "# Cantidad de numeros.\n",
    "numbers_patterns = ['\\d{1,9}\\.{0,1}\\d{0,4}']\n",
    "print('Cantidad de numeros en event_1: ', count(df, 'event_1', numbers_patterns))\n",
    "print('Cantidad de numeros en event_2: ', count(df, 'event_2', numbers_patterns))\n",
    "\n",
    "# Cantidad de fechas.\n",
    "dates_patterns = ['^\\d{4}-\\d{2}-\\d{2}$', '\\d{4}-\\d{2}-\\d{2}', '\\d{1,2}/\\d{1,2}/\\d{2}', '\\d{1}-\\d{1}']\n",
    "print('Cantidad de fechas en event_1: ', count(df, 'event_1', dates_patterns))\n",
    "print('Cantidad de fechas en event_2: ', count(df, 'event_2', dates_patterns))\n",
    "\n",
    "# normalizamos fechas\n",
    "replace_with_patterns(df, 'event_1_norm', dates_patterns, 'DATE')\n",
    "replace_with_patterns(df, 'event_2_norm', dates_patterns, 'DATE')\n",
    "print('Cantidad de fechas en event_1_norm: ', count(df, 'event_1_norm', dates_patterns))\n",
    "print('Cantidad de fechas en event_2_norm: ', count(df, 'event_2_norm', dates_patterns))\n",
    "\n",
    "# normalizamos números\n",
    "replace_with_patterns(df, 'event_1_norm', numbers_patterns, 'NUM')\n",
    "replace_with_patterns(df, 'event_2_norm', numbers_patterns, 'NUM')\n",
    "print('Cantidad de numeros en event_1_norm: ', count(df, 'event_1_norm', numbers_patterns))\n",
    "print('Cantidad de numeros en event_2_norm: ', count(df, 'event_2_norm', numbers_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el df.\n",
    "df.to_csv('../dataset/data_pos_curacion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Curación - Relaciones Temporales.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
